{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Draft - In Review"
      ],
      "metadata": {
        "id": "0CBz0lN5Q0dm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial you will see how to build a RAG application utilizing the [LangChain](https://www.langchain.com/) framework, [OpenAI](https://platform.openai.com/docs/introduction) models, and [Gradio](https://www.gradio.app/) for interface creation, we'll guide you through building a question-answering application that leverages vector databases for more accurate and informed responses. This hands-on session aims to provide a high-level understanding of these advanced concepts and demonstrate their practical application in real-world scenarios\n",
        "\n",
        "## How to run this notebook\n",
        "\n",
        "This Colab Notebook is designed to be run against your own MongoDB Atlas cluster. You can [sign up for a *free* MongoDB Atlas account](https://www.mongodb.com/cloud/atlas/register) and create a free cluster.\n",
        "\n",
        "You will also need an OpenAI API account, along with some credit. If you don't have one, you can [sign up for an OpenAI account](https://platform.openai.com/signup). Create an OpenAI API key. This requires a paid account with OpenAI, with enough credits. OpenAI API requests stop working if credit balance reaches $0.\n",
        "\n",
        "## Running in Google Colab\n",
        "\n",
        "You need to configure two secrets (using the key icon on the side of the page).\n",
        "\n",
        "Set `mongodb_connection_string` to a valid MongoDB connection string. This should include a valid username and password, _and_ a database name, like this:\n",
        "\n",
        "\n",
        "    mongodb+srv://USERNAME:PASSWORD@sandbox.abcdef.mongodb.net/DATABASE?retryWrites=true&w=majority\n",
        "\n",
        "Set `openai_api_key`, which should store the OpenAI key you can create on the [OpenAI API key page](https://platform.openai.com/api-keys).\n",
        "\n",
        "Finally, you need a [GitHub Access Token](https://github.com/settings/personal-access-tokens/new), configured with the name \"github_access_token\"\n",
        "\n",
        "## Running locally\n",
        "\n",
        "If you wish to run the notebook locally, you'll need to set the _environment variables_ `openai_api_key` and `mongodb_connection_string` to the values described in the previous section."
      ],
      "metadata": {
        "id": "n598lMXi8avw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # If we're in a colab environment, load the configured secrets:\n",
        "  from google.colab import userdata\n",
        "  openai_api_key = userdata.get('openai_api_key')\n",
        "  mongodb_connection_string = userdata.get('mongodb_connection_string')\n",
        "except ImportError:\n",
        "  # If the notebook is running outside of colab, configuration is via\n",
        "  # environment variables.\n",
        "  import os\n",
        "  openai_api_key = os.environ['openai_api_key']\n",
        "  mongodb_connection_string = os.environ['mongodb_connection_string']\n"
      ],
      "metadata": {
        "id": "ms8rGNk2uJIq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xIreOddduC_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b23cd7b-2411-4323-f03f-eef6cb12c0ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.11-py3-none-any.whl (807 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymongo\n",
            "  Downloading pymongo-4.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.2/677.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.25 (from langchain)\n",
            "  Downloading langchain_community-0.0.25-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.29 (from langchain)\n",
            "  Downloading langchain_core-0.1.29-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.6/252.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.22-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.29->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.29->langchain) (23.2)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: orjson, mypy-extensions, marshmallow, jsonpointer, dnspython, typing-inspect, pymongo, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.4 dnspython-2.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.11 langchain-community-0.0.25 langchain-core-0.1.29 langchain-text-splitters-0.0.1 langsmith-0.1.22 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.9.15 pymongo-4.6.2 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary libraries\n",
        "import sys\n",
        "!{sys.executable} -m pip install langchain pymongo\n",
        "# bs4 openai tiktoken gradio requests lxml argparse unstructured"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to divide this task into two steps:\n",
        "\n",
        "* Load some data into MongoDB and add vector embeddings.\n",
        "* Query the data using vector search and passing the results to OpenAI to interpret the results.\n",
        "\n",
        "First...\n",
        "\n",
        "## Loading some data"
      ],
      "metadata": {
        "id": "adJnePTnYqAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the following libraries:\n",
        "from pymongo import MongoClient\n",
        "from langchain_core.documents import Document\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
        "from langchain.document_loaders import DirectoryLoader, GithubFileLoader\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n"
      ],
      "metadata": {
        "id": "XORgITLRvXpw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this tutorial, we will be loading three text files from a directory using the DirectoryLoader\n",
        "# These files should be saved to a directory named sample_files :\n",
        "# \"log_example.txt\"\n",
        "\n",
        "# 2023-08-16T16:43:06.537+0000 I MONGOT [63528f5c2c4f78275d37902d-f5-u6-a0 BufferlessChangeStreamApplier] [63528f5c2c4f78275d37902d-f5-u6-a0 BufferlessChangeStreamApplier] Starting change stream from opTime=Timestamp{value=7267960339944178238, seconds=1692203884, inc=574}2023-08-16T16:43:06.543+0000 W MONGOT [63528f5c2c4f78275d37902d-f5-u6-a0 BufferlessChangeStreamApplier] [c.x.m.r.m.common.SchedulerQueue] cancelling queue batches for 63528f5c2c4f78275d37902d-f5-u6-a02023-08-16T16:43:06.544+0000 E MONGOT [63528f5c2c4f78275d37902d-f5-u6-a0 InitialSyncManager] [BufferlessInitialSyncManager 63528f5c2c4f78275d37902d-f5-u6-a0] Caught exception waiting for change stream events to be applied. Shutting down.com.xgen.mongot.replication.mongodb.common.InitialSyncException: com.mongodb.MongoCommandException: Command failed with error 286 (ChangeStreamHistoryLost): 'Executor error during getMore :: caused by :: Resume of change stream was not possible, as the resume point may no longer be in the oplog.' on server atlas-6keegs-shard-00-01.4bvxy.mongodb.net:27017.2023-08-16T16:43:06.545+0000 I MONGOT [indexing-lifecycle-3] [63528f5c2c4f78275d37902d-f5-u6-a0 ReplicationIndexManager] Transitioning from INITIAL_SYNC to INITIAL_SYNC_BACKOFF.2023-08-16T16:43:18.068+0000 I MONGOT [config-monitor] [c.x.m.config.provider.mms.ConfCaller] Conf call response has not changed. Last update date: 2023-08-16T16:43:18Z.2023-08-16T16:43:36.545+0000 I MONGOT [indexing-lifecycle-2] [63528f5c2c4f78275d37902d-f5-u6-a0 ReplicationIndexManager] Transitioning from INITIAL_SYNC_BACKOFF to INITIAL_SYNC.\n",
        "\n",
        "\n",
        "\n",
        "# \"chat_conversation.txt\"\n",
        "\n",
        "# Alfred: Hi, can you explain to me how compression works in MongoDB? Bruce: Sure! MongoDB supports compression of data at rest. It uses either zlib or snappy compression algorithms at the collection level. When data is written, MongoDB compresses and stores it compressed. When data is read, MongoDB uncompresses it before returning it. Compression reduces storage space requirements. Alfred: Interesting, that's helpful to know. Can you also tell me how indexes are stored in MongoDB? Bruce: MongoDB indexes are stored in B-trees. The internal nodes of the B-trees contain keys that point to children nodes or leaf nodes. The leaf nodes contain references to the actual documents stored in the collection. Indexes are stored in memory and also written to disk. The in-memory B-trees provide fast access for queries using the index.Alfred: Ok that makes sense. Does MongoDB compress the indexes as well?Bruce: Yes, MongoDB also compresses the index data using prefix compression. This compresses common prefixes in the index keys to save space. However, the compression is lightweight and focused on performance vs storage space. Index compression is enabled by default.Alfred: Great, that's really helpful context on how indexes are handled. One last question - when I query on a non-indexed field, how does MongoDB actually perform the scanning?Bruce: MongoDB performs a collection scan if a query does not use an index. It will scan every document in the collection in memory and on disk to select the documents that match the query. This can be resource intensive for large collections without indexes, so indexing improves query performance.Alfred: Thank you for the detailed explanations Bruce, I really appreciate you taking the time to walk through how compression and indexes work under the hood in MongoDB. Very helpful!Bruce: You're very welcome! I'm glad I could explain the technical details clearly. Feel free to reach out if you have any other MongoDB questions.\n",
        "\n",
        "# \"aerodynamics.txt\"\n",
        "\n",
        "# Boundary layer control, achieved using suction or blowing methods, can significantly reduce the aerodynamic drag on an aircraft's wing surface.The yaw angle of an aircraft, indicative of its side-to-side motion, is crucial for stability and is controlled primarily by the rudder.With advancements in computational fluid dynamics (CFD), engineers can accurately predict the turbulent airflow patterns around complex aircraft geometries, optimizing their design for better performance."
      ],
      "metadata": {
        "id": "HbgFWgFZveYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load the documents\n",
        "client = MongoClient(mongodb_connection_string)\n",
        "collection_name = \"collection_of_text_blobs\"\n",
        "collection = client.get_default_database().get_collection(collection_name)"
      ],
      "metadata": {
        "id": "fhFO1KAu6ZCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's initiatise the directory loader\n",
        "loader = DirectoryLoader( './sample_files', glob=\"./*.txt\", show_progress=True)\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "EJm8lUkQ6gcW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "9ab72d6a-64e3-4527-eda1-26cea0b3dd56"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Directory not found: './sample_files'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e207fd07fe59>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's initiatise the directory loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDirectoryLoader\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'./sample_files'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./*.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/document_loaders/directory.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Directory not found: '{self.path}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected directory, got file: '{self.path}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Directory not found: './sample_files'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the OpenAI Embedding Model we want to use for the source data\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=key_param.openai_api_key)"
      ],
      "metadata": {
        "id": "iE3bHb4k6g9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise the VectorStore\n",
        "vectorStore = MongoDBAtlasVectorSearch.from_documents( data, embeddings, collection=collection )"
      ],
      "metadata": {
        "id": "qvk433ka6hZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Atlas Search Index\n",
        "{\n",
        "  \"type\": \"vectorSearch\",\n",
        "  \"fields\": [{\n",
        "    \"path\": \"embedding\",\n",
        "    \"numDimensions\": 1536,\n",
        "    \"similarity\": \"cosine\",\n",
        "    \"type\": \"vector\"\n",
        "  }]\n",
        "}"
      ],
      "metadata": {
        "id": "fzCFW5HU6h0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performing vector search using Atlas Vector Search\n"
      ],
      "metadata": {
        "id": "JgAEymIh7EOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the OpenAI Embedding Model we want to use\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "OCbldSRQ69HU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef9815fa-a56d-44d1-bc24-259b930fc326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Vector Store\n",
        "vectorStore = MongoDBAtlasVectorSearch( collection, embeddings )"
      ],
      "metadata": {
        "id": "6jvqtu9469WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that performs semantic similarity search using Atlas Vector Search\n",
        "# Note we are including this step only to highlight the differences between output of only semantic search vs output generated with RAG architecture using RetrieverQA\n",
        "\n",
        "def query_data(query):\n",
        "    # Convert question to vector using OpenAI embeddings\n",
        "    # Perform Atlas Vector Search using Langchain's vectorStore\n",
        "    # similarity_search returns MongoDB documents most similar to the query\n",
        "\n",
        "    docs = vectorStore.similarity_search(query, K=1)\n",
        "    as_output = docs[0].page_content\n",
        "\n",
        "    # Define a function that uses a retrieval-based augmentation to perform question-answering on the data\n",
        "\n",
        "    # Leveraging Atlas Vector Search paired with Langchain's QARetriever\n",
        "\n",
        "    # Define the LLM that we want to use -- note that this is the Language Generation Model and NOT an Embedding Model\n",
        "    # If it's not specified (for example like in the code below),\n",
        "    # then the default OpenAI model used in LangChain is OpenAI GPT-3.5-turbo, as of August 30, 2023\n",
        "\n",
        "    llm = OpenAI(openai_api_key=openai_api_key, temperature=0)\n",
        "\n",
        "\n",
        "    # Get VectorStoreRetriever: Specifically, Retriever for MongoDB VectorStore.\n",
        "    # Implements _get_relevant_documents which retrieves documents relevant to a query.\n",
        "    retriever = vectorStore.as_retriever()\n",
        "\n",
        "    # Load \"stuff\" documents chain. Stuff documents chain takes a list of documents,\n",
        "    # inserts them all into a prompt and passes that prompt to an LLM.\n",
        "\n",
        "    qa = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=retriever)\n",
        "\n",
        "    # Execute the chain\n",
        "\n",
        "    retriever_output = qa.run(query)\n",
        "\n",
        "\n",
        "    # Return Atlas Vector Search output, and output generated using RAG Architecture\n",
        "    return as_output, retriever_output"
      ],
      "metadata": {
        "id": "-tEOobe37SiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "siOgYXH57SyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a web interface for the app using Gradio\n",
        "\n",
        "import gradio as gr\n",
        "from gradio.themes.base import Base\n",
        "\n",
        "with gr.Blocks(theme=Base(), title=\"Question Answering App using Vector Search + RAG\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # Question Answering App using Atlas Vector Search + RAG Architecture\n",
        "        \"\"\")\n",
        "    textbox = gr.Textbox(label=\"Enter your Question:\")\n",
        "    with gr.Row():\n",
        "        button = gr.Button(\"Submit\", variant=\"primary\")\n",
        "    with gr.Column():\n",
        "        output1 = gr.Textbox(lines=1, max_lines=10, label=\"Output with just Atlas Vector Search (returns text field as is):\")\n",
        "        output2 = gr.Textbox(lines=1, max_lines=10, label=\"Output generated by chaining Atlas Vector Search to Langchain's RetrieverQA + OpenAI LLM:\")\n",
        "\n",
        "# Call query_data function upon clicking the Submit button\n",
        "\n",
        "    button.click(query_data, textbox, outputs=[output1, output2])\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "m_eOdmzZ7pml",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "outputId": "1b8a1c8f-20db-4ba1-ce46-f94060d37372"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "\n",
            "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7861, \"/\", \"100%\", 500, false, window.element)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output\n",
        "\n"
      ],
      "metadata": {
        "id": "Tao1PP_D7-IK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Log analysis example"
      ],
      "metadata": {
        "id": "Ihr2uNHM7qJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat conversation example"
      ],
      "metadata": {
        "id": "CIti45AM7qZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment analysis example"
      ],
      "metadata": {
        "id": "f9xyaErR7TCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precise answer retrieval example"
      ],
      "metadata": {
        "id": "LxKjyuSA69lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps"
      ],
      "metadata": {
        "id": "VC2XiNHb8UJm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QpmBxmdV8XGy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}